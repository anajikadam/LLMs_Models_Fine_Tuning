{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tuning GPT-3 Using the OpenAI API and Python**"
      ],
      "metadata": {
        "id": "H_PoJnnHvP-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPT models that can be fine-tuned include Ada, Babbage, Curie, and Davinci. These models belong to the GPT-3 family. Also, it is important to note that fine-tuning is currently not available for more recent GPT-3.5-turbo models or other GPT-4."
      ],
      "metadata": {
        "id": "nf86ZtUCvaJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification and conditional generation are the two types of problems that can benefit from fine-tuning a language model like GPT-3\n",
        "\n",
        "### Classification\n",
        "For classification problems, each input in the prompt is assigned one of the predefined classes, and some of the cases are illustrated below:\n",
        "\n",
        "- Ensuring truthful statements: If a company wants to verify that ads on their website mention the correct product and company, a classifier can be fine-tuned to filter out incorrect ads, ensuring the model isn't making things up.\n",
        "- Sentiment analysis: This involves classifying text based on sentiment, such as positive, negative, or neutral.\n",
        "- Email triage categorization: To sort incoming emails into one of many predefined categories, those categories can be converted into numbers, which work well well for up to ~500 categories.\n",
        "\n",
        "\n",
        "### Conditional generation\n",
        "The problems in this category involve generating content based on a given input. Applications include paraphrasing, summarizing, entity extraction, product description writing, virtual assistants (chatbots), and more.\n",
        "\n",
        "Examples include:\n",
        "\n",
        "- Creating engaging ads from Wikipedia articles. In this generative use case, ensure that the samples provided are high-quality, as the fine-tuned model will attempt to imitate the style (and mistakes) of the examples.\n",
        "- Entity extraction. This task is akin to a language transformation problem. Improve performance by sorting extracted entities alphabetically or in the same order as they appear in the original text.\n",
        "- Customer support chatbot. A chatbot typically includes relevant context about the conversation (order details), a summary of the conversation so far, and the most recent messages.\n",
        "- Product description based on technical properties. Convert input data into a natural language to achieve superior performance in this context.\n"
      ],
      "metadata": {
        "id": "i2DhLe7ZvgZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2USWjXi-wbGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/docs/guides/fine-tuning"
      ],
      "metadata": {
        "id": "dSQm94LkvfnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://openai.com/pricing"
      ],
      "metadata": {
        "id": "e5AE9GWk8K-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine-tune the GPT-3 model for a question-answering scenario\n",
        "\n",
        "A structured question-answer pattern designed to help the model understand the task the model needs to perform. A consistent format is maintained for each pair of questions and answers across the entire training and testing data.\n",
        "\n",
        "```\n",
        "{\n",
        "\"prompt\": \"my prompt ->\",\n",
        "\"completion\": \"the answer of the prompt. \\n\"\n",
        "}\n",
        "```\n",
        "\n",
        "“prompt” is the input text read and processed by the model. The main separator is the arrow sign (->) to delineate the prompt from the expected response.\n",
        "“completion” is the expected response to the prompt. A backslash “\\n” sign is used as a stop sequence to indicate the end of the answer.\n",
        "\n",
        "Generate both the training and the validation dataset, as shown below. These prompts and completions have been generated using ChatGPT"
      ],
      "metadata": {
        "id": "_OKM_tVJwku0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The conversational chat format is required to fine-tune gpt-3.5-turbo. For babbage-002 and davinci-002, you can follow the prompt completion pair format as shown below."
      ],
      "metadata": {
        "id": "lyQRK0M79HDI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww_ZL5_UvPfV"
      },
      "outputs": [],
      "source": [
        "training_data = [\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the capital of France?->\",\n",
        "    \t\"completion\": \"\"\" The capital of France is Paris.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the primary function of the heart?->\",\n",
        "    \t\"completion\": \"\"\" The primary function of the heart is to pump blood throughout the body.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is photosynthesis?->\",\n",
        "    \t\"completion\": \"\"\" Photosynthesis is the process by which green plants and some other organisms convert sunlight into chemical energy stored in the form of glucose.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"Who wrote the play 'Romeo and Juliet'?->\",\n",
        "    \t\"completion\": \"\"\" William Shakespeare wrote the play 'Romeo and Juliet'.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"Which element has the atomic number 1?->\",\n",
        "    \t\"completion\": \"\"\" Hydrogen has the atomic number 1.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the largest planet in our solar system?->\",\n",
        "    \t\"completion\": \"\"\" Jupiter is the largest planet in our solar system.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the freezing point of water in Celsius?->\",\n",
        "    \t\"completion\": \"\"\" The freezing point of water in Celsius is 0 degrees.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the square root of 144?->\",\n",
        "    \t\"completion\": \"\"\" The square root of 144 is 12.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"Who is the author of 'To Kill a Mockingbird'?->\",\n",
        "    \t\"completion\": \"\"\" The author of 'To Kill a Mockingbird' is Harper Lee.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the smallest unit of life?->\",\n",
        "    \t\"completion\": \"\"\" The smallest unit of life is the cell.\\n\"\"\"\n",
        "\t},\n",
        "    {\n",
        "    \t\"prompt\": \"What is Capital of Pune district?->\",\n",
        "    \t\"completion\": \"\"\" Pune city is Capital of Pune district\\n\"\"\"\n",
        "\t},\n",
        "    {\n",
        "    \t\"prompt\": \"What is Capital of Sindhudurg district?->\",\n",
        "    \t\"completion\": \"\"\" Kudal is Capital of Sindhudurg district\\n\"\"\"\n",
        "\t}\n",
        "]\n",
        "\n",
        "validation_data = [\n",
        "\t{\n",
        "    \t\"prompt\": \"Which gas do plants use for photosynthesis?->\",\n",
        "    \t\"completion\": \"\"\" Plants use carbon dioxide for photosynthesis.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What are the three primary colors of light?->\",\n",
        "    \t\"completion\": \"\"\" The three primary colors of light are red, green, and blue.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"Who discovered penicillin?->\",\n",
        "    \t\"completion\": \"\"\" Sir Alexander Fleming discovered penicillin.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the chemical formula for water?->\",\n",
        "    \t\"completion\": \"\"\" The chemical formula for water is H2O.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the largest country by land area?->\",\n",
        "    \t\"completion\": \"\"\" Russia is the largest country by land area.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the speed of light in a vacuum?->\",\n",
        "    \t\"completion\": \"\"\" The speed of light in a vacuum is approximately 299,792 kilometers per second.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the currency of Japan?->\",\n",
        "    \t\"completion\": \"\"\" The currency of Japan is the Japanese Yen.\\n\"\"\"\n",
        "\t},\n",
        "\t{\n",
        "    \t\"prompt\": \"What is the smallest bone in the human body?->\",\n",
        "    \t\"completion\": \"\"\" The stapes, located in the middle ear, is the smallest bone in the human body.\\n\"\"\"\n",
        "\t},\n",
        "    {\n",
        "    \t\"prompt\": \"What is Capital of Ratnagiri district?->\",\n",
        "    \t\"completion\": \"\"\" Mandangadh city is Capital of Ratnagiri district\\n\"\"\"\n",
        "\t}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqAEYTeFxqp1",
        "outputId": "910d24d1-bef2-477e-fa21-26586bc71bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/225.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "y7OrDm9mxzMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "jacSIXAGxqoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "training_file_name = \"training_data.jsonl\"\n",
        "validation_file_name = \"validation_data.jsonl\"\n",
        "\n",
        "def prepare_data(dictionary_data, final_file_name):\n",
        "    with open(final_file_name, 'w') as outfile:\n",
        "        for entry in dictionary_data:\n",
        "            json.dump(entry, outfile)\n",
        "            outfile.write('\\n')\n",
        "\n",
        "prepare_data(training_data, \"training_data.jsonl\")\n",
        "prepare_data(validation_data, \"validation_data.jsonl\")"
      ],
      "metadata": {
        "id": "NunMPzN6xqle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the two datasets to the OpenAI developer account"
      ],
      "metadata": {
        "id": "cFvk4UUb7-hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_file_id = client.files.create(\n",
        "  file=open(training_file_name, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "validation_file_id = client.files.create(\n",
        "  file=open(validation_file_name, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "print(f\"Training File ID: {training_file_id}\")\n",
        "print(f\"Validation File ID: {validation_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS5IpHn6xqim",
        "outputId": "332c8d31-69c7-46f1-ed8b-1a97177f14cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training File ID: FileObject(id='file-zA67GR5mqAnvMfgMQcuMgWis', bytes=1534, created_at=1706594461, filename='training_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
            "Validation File ID: FileObject(id='file-46UVHFDaFc4a0HpInPtqOAWH', bytes=1160, created_at=1706594462, filename='validation_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a fine-tuning job\n",
        "This fine-tuning process is highly inspired by the openai-cookbook performing fine-tuning on Microsoft Azure.\n",
        "\n",
        "To perform the fine-tuning we will use the following two steps: (1) define hyperparameters, and (2) trigger the fine-tuning.\n",
        "\n",
        "We will fine-tune the davinci model and run it for 15 epochs using a batch size of 3 and a learning rate multiplier of 0.3 using the training and validation datasets."
      ],
      "metadata": {
        "id": "QYXzjjmK9ZEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.fine_tuning.jobs.create(\n",
        "                                        training_file=training_file_id.id,\n",
        "                                        validation_file=validation_file_id.id,\n",
        "                                        model=\"davinci-002\",\n",
        "                                        hyperparameters={\n",
        "                                            \"n_epochs\": 15,\n",
        "                                            \"batch_size\": 3,\n",
        "                                            \"learning_rate_multiplier\": 0.3\n",
        "                                        }\n",
        "                                        )\n",
        "job_id = response.id\n",
        "status = response.status\n",
        "\n",
        "print(f'Fine-tunning model with jobID: {job_id}.')\n",
        "print(f\"Training Response: {response}\")\n",
        "print(f\"Training Status: {status}\")"
      ],
      "metadata": {
        "id": "1pv17lBE9emy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above generates the following information for the jobID (`ftjob-WsCjQMEmSvRMFCbhdTqxZzOf`), the training response, and the training status (pending)"
      ],
      "metadata": {
        "id": "pm7fV2ar-Ids"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pending status does not provide any relevant information. However, we can have more insight into the training process by running the following code:"
      ],
      "metadata": {
        "id": "MT8rMrtu-Nnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "import datetime\n",
        "\n",
        "\n",
        "def signal_handler(sig, frame):\n",
        "    status = client.fine_tuning.jobs.retrieve(job_id).status\n",
        "    print(f\"Stream interrupted. Job is still {status}.\")\n",
        "    return\n",
        "\n",
        "\n",
        "print(f\"Streaming events for the fine-tuning job: {job_id}\")\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id)\n",
        "try:\n",
        "    for event in events:\n",
        "        print(\n",
        "            f'{datetime.datetime.fromtimestamp(event.created_at)} {event.message}'\n",
        "        )\n",
        "except Exception:\n",
        "    print(\"Stream interrupted (client disconnected).\")"
      ],
      "metadata": {
        "id": "cnsT42Aw9fPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the epochs are generated below, along with the status of the fine-tuning, which is succeeded"
      ],
      "metadata": {
        "id": "-UaN7JhR-R6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the fine-tuning job status\n",
        "Let's verify that our operation was successful, and additionally, we can examine all the fine-tuning operations by using a list operation."
      ],
      "metadata": {
        "id": "LrAUC7Fe-UOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "status = client.fine_tuning.jobs.retrieve(job_id).status\n",
        "if status not in [\"succeeded\", \"failed\"]:\n",
        "    print(f\"Job not in terminal status: {status}. Waiting.\")\n",
        "    while status not in [\"succeeded\", \"failed\"]:\n",
        "        time.sleep(2)\n",
        "        status = client.fine_tuning.jobs.retrieve(job_id).status\n",
        "        print(f\"Status: {status}\")\n",
        "else:\n",
        "    print(f\"Finetune job {job_id} finished with status: {status}\")\n",
        "print(\"Checking other finetune jobs in the subscription.\")\n",
        "result = client.fine_tuning.jobs.list()\n",
        "print(f\"Found {len(result.data)} finetune jobs.\")"
      ],
      "metadata": {
        "id": "Y2YgAoFv9fMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation of the model\n",
        "Finally, the fine-tuned model can be retrieved from the “fine_tuned_model” attribute. The following print statement shows that the name of the final mode is: `ft:davinci-002:personal::8gKnyxn3`"
      ],
      "metadata": {
        "id": "53UBLF8J-Y5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the finetuned model\n",
        "fine_tuned_model = result.data[0].fine_tuned_model\n",
        "print(fine_tuned_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "t9rcc_l19fJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this model, we can run queries to validate its results by providing a prompt, the model name, and creating a query with the openai.Completion.create() function. The result is retrieved from the answer dictionary as follows:"
      ],
      "metadata": {
        "id": "2bcSx_Py-elB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = \"Which part is the smallest bone in the entire human body?\"\n",
        "answer = client.completions.create(\n",
        "  model=fine_tuned_model,\n",
        "  prompt=new_prompt\n",
        ")\n",
        "\n",
        "print(answer.choices[0].text)\n",
        "\n",
        "new_prompt = \"Which type of gas is utilized by plants during the process of photosynthesis?\"\n",
        "answer = client.completions.create(\n",
        "  model=fine_tuned_model,\n",
        "  prompt=new_prompt\n",
        ")\n",
        "\n",
        "print(answer.choices[0].text)"
      ],
      "metadata": {
        "id": "7SzCeUnA9fGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the prompts are not written exactly the same as in the validation dataset, the model still managed to map them to the correct answers. The answers to the previous requests are shown below."
      ],
      "metadata": {
        "id": "IWdKyWxQ-jpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With very few training samples, we managed to have a decent fine-tuned model. Better results can be achieved with a larger training size."
      ],
      "metadata": {
        "id": "cKpkqklx-nl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/fine-tuning-gpt-3-using-the-open-ai-api-and-python"
      ],
      "metadata": {
        "id": "OPao3Inu-wca"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_oU5YVL9fEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zayewz0D9fBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}